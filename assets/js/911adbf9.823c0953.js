"use strict";(self.webpackChunkvirtualclient=self.webpackChunkvirtualclient||[]).push([[8030],{3905:(t,e,n)=>{n.d(e,{Zo:()=>p,kt:()=>c});var a=n(7294);function r(t,e,n){return e in t?Object.defineProperty(t,e,{value:n,enumerable:!0,configurable:!0,writable:!0}):t[e]=n,t}function l(t,e){var n=Object.keys(t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(t);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),n.push.apply(n,a)}return n}function i(t){for(var e=1;e<arguments.length;e++){var n=null!=arguments[e]?arguments[e]:{};e%2?l(Object(n),!0).forEach((function(e){r(t,e,n[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(n,e))}))}return t}function o(t,e){if(null==t)return{};var n,a,r=function(t,e){if(null==t)return{};var n,a,r={},l=Object.keys(t);for(a=0;a<l.length;a++)n=l[a],e.indexOf(n)>=0||(r[n]=t[n]);return r}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(a=0;a<l.length;a++)n=l[a],e.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(t,n)&&(r[n]=t[n])}return r}var m=a.createContext({}),d=function(t){var e=a.useContext(m),n=e;return t&&(n="function"==typeof t?t(e):i(i({},e),t)),n},p=function(t){var e=d(t.components);return a.createElement(m.Provider,{value:e},t.children)},u={inlineCode:"code",wrapper:function(t){var e=t.children;return a.createElement(a.Fragment,{},e)}},s=a.forwardRef((function(t,e){var n=t.components,r=t.mdxType,l=t.originalType,m=t.parentName,p=o(t,["components","mdxType","originalType","parentName"]),s=d(n),c=r,k=s["".concat(m,".").concat(c)]||s[c]||u[c]||l;return n?a.createElement(k,i(i({ref:e},p),{},{components:n})):a.createElement(k,i({ref:e},p))}));function c(t,e){var n=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var l=n.length,i=new Array(l);i[0]=s;var o={};for(var m in e)hasOwnProperty.call(e,m)&&(o[m]=e[m]);o.originalType=t,o.mdxType="string"==typeof t?t:r,i[1]=o;for(var d=2;d<l;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}s.displayName="MDXCreateElement"},5930:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>m,contentTitle:()=>i,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>d});var a=n(7462),r=(n(7294),n(3905));const l={},i="SuperBenchmark Workload Metrics",o={unversionedId:"workloads/superbenchmark/SuperBenchmarkMetrics",id:"workloads/superbenchmark/SuperBenchmarkMetrics",title:"SuperBenchmark Workload Metrics",description:"The following document illustrates the type of results that are emitted by the SuperBenchmark workload and captured by the",source:"@site/docs/workloads/superbenchmark/SuperBenchmarkMetrics.md",sourceDirName:"workloads/superbenchmark",slug:"/workloads/superbenchmark/SuperBenchmarkMetrics",permalink:"/docs/workloads/superbenchmark/SuperBenchmarkMetrics",draft:!1,editUrl:"https://github.com/microsoft/VirtualClient/docs/workloads/superbenchmark/SuperBenchmarkMetrics.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"SuperBenchmark Workload Suite",permalink:"/docs/workloads/superbenchmark/"},next:{title:"SuperBenchmark Workload Profiles",permalink:"/docs/workloads/superbenchmark/SuperBenchmarkProfiles"}},m={},d=[{value:"System Metrics",id:"system-metrics",level:3},{value:"Workload-Specific Metrics",id:"workload-specific-metrics",level:3},{value:"Computation Benchmarks",id:"computation-benchmarks",level:2},{value:"<code>kernel-launch</code>",id:"kernel-launch",level:3},{value:"Introduction",id:"introduction",level:4},{value:"Metrics",id:"metrics",level:4},{value:"<code>gemm-flops</code>",id:"gemm-flops",level:3},{value:"Introduction",id:"introduction-1",level:4},{value:"Metrics",id:"metrics-1",level:4},{value:"<code>matmul</code>",id:"matmul",level:3},{value:"Introduction",id:"introduction-2",level:4},{value:"Metrics",id:"metrics-2",level:4},{value:"<code>cublas-function</code>",id:"cublas-function",level:3},{value:"<code>cudnn-function</code>",id:"cudnn-function",level:3},{value:"<code>tensorrt-inference</code>",id:"tensorrt-inference",level:3},{value:"Introduction",id:"introduction-3",level:4},{value:"Metrics",id:"metrics-3",level:4},{value:"<code>ort-inference</code>",id:"ort-inference",level:3},{value:"Introduction",id:"introduction-4",level:4},{value:"Metrics",id:"metrics-4",level:4},{value:"Communication Benchmarks",id:"communication-benchmarks",level:2},{value:"<code>mem-bw</code>",id:"mem-bw",level:3},{value:"Introduction",id:"introduction-5",level:4},{value:"Metrics",id:"metrics-5",level:4},{value:"<code>gpu-copy-bw</code>",id:"gpu-copy-bw",level:3},{value:"Metrics",id:"metrics-6",level:4},{value:"<code>ib-loopback</code>",id:"ib-loopback",level:3},{value:"Introduction",id:"introduction-6",level:4},{value:"Metrics",id:"metrics-7",level:4},{value:"<code>nccl-bw</code> / <code>rccl-bw</code>",id:"nccl-bw--rccl-bw",level:3},{value:"Introduction",id:"introduction-7",level:4},{value:"Metrics",id:"metrics-8",level:4},{value:"<code>tcp-connectivity</code>",id:"tcp-connectivity",level:3},{value:"Introduction",id:"introduction-8",level:4},{value:"Metrics",id:"metrics-9",level:4},{value:"<code>gpcnet-network-test</code> / <code>gpcnet-network-load-test</code>",id:"gpcnet-network-test--gpcnet-network-load-test",level:3},{value:"Introduction",id:"introduction-9",level:4},{value:"Metrics",id:"metrics-10",level:4},{value:"<code>ib-traffic</code>",id:"ib-traffic",level:3},{value:"Introduction",id:"introduction-10",level:4},{value:"Metrics",id:"metrics-11",level:4},{value:"Computation-communication Benchmarks",id:"computation-communication-benchmarks",level:2},{value:"<code>computation-communication-overlap</code>",id:"computation-communication-overlap",level:3},{value:"Introduction",id:"introduction-11",level:4},{value:"Metrics",id:"metrics-12",level:4},{value:"<code>sharding-matmul</code>",id:"sharding-matmul",level:3},{value:"Introduction",id:"introduction-12",level:4},{value:"Metrics",id:"metrics-13",level:4},{value:"Storage Benchmarks",id:"storage-benchmarks",level:2},{value:"<code>disk-benchmark</code>",id:"disk-benchmark",level:3},{value:"Introduction",id:"introduction-13",level:4},{value:"Metrics",id:"metrics-14",level:4},{value:"PyTorch Model Benchmarks",id:"pytorch-model-benchmarks",level:2},{value:"<code>gpt_models</code>",id:"gpt_models",level:3},{value:"Introduction",id:"introduction-14",level:4},{value:"Metrics",id:"metrics-15",level:4},{value:"<code>bert_models</code>",id:"bert_models",level:3},{value:"Introduction",id:"introduction-15",level:4},{value:"Metrics",id:"metrics-16",level:4},{value:"<code>lstm_models</code>",id:"lstm_models",level:3},{value:"Introduction",id:"introduction-16",level:4},{value:"Metrics",id:"metrics-17",level:4},{value:"<code>cnn_models</code>",id:"cnn_models",level:3},{value:"Introduction",id:"introduction-17",level:4},{value:"Metrics",id:"metrics-18",level:4},{value:"id: docker-benchmarks",id:"id-docker-benchmarks",level:2},{value:"ROCm ONNXRuntime Model Benchmarks",id:"rocm-onnxruntime-model-benchmarks",level:2},{value:"<code>ort-models</code>",id:"ort-models",level:3},{value:"Introduction",id:"introduction-18",level:4},{value:"Metrics",id:"metrics-19",level:4}],p={toc:d};function u(t){let{components:e,...n}=t;return(0,r.kt)("wrapper",(0,a.Z)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"superbenchmark-workload-metrics"},"SuperBenchmark Workload Metrics"),(0,r.kt)("p",null,"The following document illustrates the type of results that are emitted by the SuperBenchmark workload and captured by the\nVirtual Client for net impact analysis."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks"},"SuperBenchmark MicroBenchmarks")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/model-benchmarks"},"SuperBenchmark ModelBenchmarks"),"  "),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/docker-benchmarks"},"SuperBenchmark DockerBenchmarks"),"  ")),(0,r.kt)("h3",{id:"system-metrics"},"System Metrics"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"./PerformanceCounterMetrics.md"},"Performance Counters")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"./PowerMetrics.md"},"Power/Temperature Measurements"),"  ")),(0,r.kt)("h3",{id:"workload-specific-metrics"},"Workload-Specific Metrics"),(0,r.kt)("p",null,"The following metrics are captured during the operations of the SuperBenchmark workload."),(0,r.kt)("h1",{id:"micro-benchmarks"},"Micro Benchmarks"),(0,r.kt)("h2",{id:"computation-benchmarks"},"Computation Benchmarks"),(0,r.kt)("h3",{id:"kernel-launch"},(0,r.kt)("inlineCode",{parentName:"h3"},"kernel-launch")),(0,r.kt)("h4",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"Measure GPU kernel launch latency,\nwhich is defined as the time range from the beginning of the launch API call to the beginning of the kernel execution."),(0,r.kt)("h4",{id:"metrics"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kernel-launch/event_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Launch latency measured in GPU time.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kernel-launch/wall_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Launch latency measured in CPU time.")))),(0,r.kt)("h3",{id:"gemm-flops"},(0,r.kt)("inlineCode",{parentName:"h3"},"gemm-flops")),(0,r.kt)("h4",{id:"introduction-1"},"Introduction"),(0,r.kt)("p",null,"Measure the GPU GEMM FLOPS for different float and int data types, with or without Tensor Core (XDLOPS),\nperformed by NVIDIA ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/NVIDIA/cutlass/tree/ccb697bac77fcc898e9c897b2c90aa5b60ac72fb"},"cutlass"),"\nor AMD ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ROCmSoftwarePlatform/rocBLAS/tree/develop/clients/benchmarks"},"rocblas-bench"),"."),(0,r.kt)("h4",{id:"metrics-1"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp64_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM float64 peak FLOPS.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp32_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM float32 peak FLOPS.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp16_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp64_tc_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM float64 peak FLOPS with NVIDIA Tensor Core.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/tf32_tc_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM tensor-float32 peak FLOPS with NVIDIA Tensor Core.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp16_tc_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS with NVIDIA Tensor Core.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/bf16_tc_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM bfloat16 peak FLOPS with NVIDIA Tensor Core.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/int8_tc_iops"),(0,r.kt)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM int8 peak IOPS with NVIDIA Tensor Core.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/int4_tc_iops"),(0,r.kt)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM int4 peak IOPS with NVIDIA Tensor Core.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp32_xdlops_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM tensor-float32 peak FLOPS with AMD XDLOPS.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp16_xdlops_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS with AMD XDLOPS.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/bf16_xdlops_flops"),(0,r.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM bfloat16 peak FLOPS with AMD XDLOPS.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gemm-flops/int8_xdlops_iops"),(0,r.kt)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,r.kt)("td",{parentName:"tr",align:null},"GEMM int8 peak IOPS with AMD XDLOPS.")))),(0,r.kt)("h3",{id:"matmul"},(0,r.kt)("inlineCode",{parentName:"h3"},"matmul")),(0,r.kt)("h4",{id:"introduction-2"},"Introduction"),(0,r.kt)("p",null,"Large scale matmul operation using ",(0,r.kt)("inlineCode",{parentName:"p"},"torch.matmul")," with one GPU."),(0,r.kt)("h4",{id:"metrics-2"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"pytorch-matmul/nosharding_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Time of pure matmul operation.")))),(0,r.kt)("h3",{id:"cublas-function"},(0,r.kt)("inlineCode",{parentName:"h3"},"cublas-function")),(0,r.kt)("p",null,"TODO"),(0,r.kt)("h3",{id:"cudnn-function"},(0,r.kt)("inlineCode",{parentName:"h3"},"cudnn-function")),(0,r.kt)("p",null,"TODO"),(0,r.kt)("h3",{id:"tensorrt-inference"},(0,r.kt)("inlineCode",{parentName:"h3"},"tensorrt-inference")),(0,r.kt)("h4",{id:"introduction-3"},"Introduction"),(0,r.kt)("p",null,"Inference PyTorch/ONNX models on NVIDIA GPUs with ",(0,r.kt)("a",{parentName:"p",href:"https://developer.nvidia.com/tensorrt"},"TensorRT"),".\nCurrently the following models are supported:"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,\nmnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,\nresnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,\nsqueezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19")),(0,r.kt)("h4",{id:"metrics-3"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_gpu_time_mean"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"The mean GPU latency to execute the kernels for a query.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_gpu_time_99"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"The 99th percentile GPU latency to execute the kernels for a query.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_host_time_mean"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"The mean H2D, GPU, and D2H latency to execute the kernels for a query.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_host_time_99"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"The 99th percentile H2D, GPU, and D2H latency to execute the kernels for a query.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_end_to_end_time_mean"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"The mean duration from when the H2D of a query is called to when the D2H of the same query is completed.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_end_to_end_time_99"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"The P99 duration from when the H2D of a query is called to when the D2H of the same query is completed.")))),(0,r.kt)("h3",{id:"ort-inference"},(0,r.kt)("inlineCode",{parentName:"h3"},"ort-inference")),(0,r.kt)("h4",{id:"introduction-4"},"Introduction"),(0,r.kt)("p",null,"Inference performance of the torchvision models using ONNXRuntime. Currently the following models are supported:"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,\nmnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,\nresnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,\nsqueezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19")),(0,r.kt)("h4",{id:"metrics-4"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ort-inference/{precision}_{model}_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"The mean latency to execute one batch of inference.")))),(0,r.kt)("h2",{id:"communication-benchmarks"},"Communication Benchmarks"),(0,r.kt)("h3",{id:"mem-bw"},(0,r.kt)("inlineCode",{parentName:"h3"},"mem-bw")),(0,r.kt)("h4",{id:"introduction-5"},"Introduction"),(0,r.kt)("p",null,"Measure the memory copy bandwidth across PCI-e and memory copy bandwidth between GPUs,\nperformed by ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/NVIDIA/cuda-samples/tree/master/Samples/bandwidthTest"},"NVIDIA"),"\nor ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ROCm-Developer-Tools/HIP/tree/master/samples/1_Utils/hipBusBandwidth"},"AMD")," bandwidth test tool."),(0,r.kt)("h4",{id:"metrics-5"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"mem-bw/h2d_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Host to device copy bandwidth.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"mem-bw/d2h_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Device to host copy bandwidth.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"mem-bw/d2d_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Device to device copy bandwidth.")))),(0,r.kt)("h3",{id:"gpu-copy-bw"},(0,r.kt)("inlineCode",{parentName:"h3"},"gpu-copy-bw")),(0,r.kt)("p",null,"Measure the memory copy bandwidth performed by GPU SM/DMA engine, including device-to-host, host-to-device and device-to-device."),(0,r.kt)("h4",{id:"metrics-6"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cpu","_","to","_","gpu","[0-9]","+","_","by","_","gpu","[0-9]","+","_","using","_","(sm","|","dma)","_","under_numa","[0-9]","+_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The bandwidth reading from all NUMA nodes' host memory using DMA engine or GPU SM by all GPUs.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","cpu","_","by","_","gpu","[0-9]","+","_","using","_","(sm","|","dma)","_","under_numa","[0-9]","+_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The bandwidth writing to all NUMA nodes' host memory using DMA engine or GPU SM by all GPUs.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to_gpu","[0-9]","+","_","by","_","gpu","[0-9]","+","_","using","_","(sm","|","dma)","_","under_numa","[0-9]","+_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The bandwidth reading from  or writing to all GPUs using DMA engine or GPU SM by all GPUs with peer communication enabled.")))),(0,r.kt)("h3",{id:"ib-loopback"},(0,r.kt)("inlineCode",{parentName:"h3"},"ib-loopback")),(0,r.kt)("h4",{id:"introduction-6"},"Introduction"),(0,r.kt)("p",null,"Measure the InfiniBand loopback verbs bandwidth, performed by\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/linux-rdma/perftest/tree/7504ce48ac396a02f4d00de359257b2cb8458f06"},"OFED performance tests"),"."),(0,r.kt)("h4",{id:"metrics-7"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,r.kt)("em",{parentName:"td"},"write"),"${msg_size}_ib","[0-9]","_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"InfiniBand loopback write bandwidth with given message size.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,r.kt)("em",{parentName:"td"},"read"),"${msg_size}_ib","[0-9]","_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"InfiniBand loopback read bandwidth with given message size.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,r.kt)("em",{parentName:"td"},"send"),"${msg_size}_ib","[0-9]","_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"InfiniBand loopback send bandwidth with given message size.")))),(0,r.kt)("h3",{id:"nccl-bw--rccl-bw"},(0,r.kt)("inlineCode",{parentName:"h3"},"nccl-bw")," / ",(0,r.kt)("inlineCode",{parentName:"h3"},"rccl-bw")),(0,r.kt)("h4",{id:"introduction-7"},"Introduction"),(0,r.kt)("p",null,"Measure the performance of NCCL/RCCL operations,\nperformed by ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/NVIDIA/nccl-tests/tree/44df0bf010dcc95e840ca0fb7466c67cff3f1f0f"},"nccl-tests"),"\nor ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ROCmSoftwarePlatform/rccl-tests/tree/dc1ad4853d7ec738387d42a75a58a98d7af00c7b"},"rccl-tests"),".\nSupport the following operations currently: allreduce, allgather, broadcast, reduce, reducescatter, alltoall."),(0,r.kt)("h4",{id:"metrics-8"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,r.kt)("td",{parentName:"tr",align:null},"NCCL operation lantency with given message size.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_algbw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"NCCL operation algorithm bandwidth with given message size.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_busbw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"NCCL operation bus bandwidth with given message size.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,r.kt)("td",{parentName:"tr",align:null},"RCCL operation lantency with given message size.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_algbw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"RCCL operation algorithm bandwidth with given message size.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_busbw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"RCCL operation bus bandwidth with given message size.")))),(0,r.kt)("h3",{id:"tcp-connectivity"},(0,r.kt)("inlineCode",{parentName:"h3"},"tcp-connectivity")),(0,r.kt)("h4",{id:"introduction-8"},"Introduction"),(0,r.kt)("p",null,"Test the TCP connectivity between current node and nodes in the hostfile,\nperformed by ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/zhengxiaowai/tcping"},"tcping")),(0,r.kt)("h4",{id:"metrics-9"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Metrics"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_successed_count"),(0,r.kt)("td",{parentName:"tr",align:null},"count"),(0,r.kt)("td",{parentName:"tr",align:null},"successed times of tcp connections between current node and other nodes")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_failed_count"),(0,r.kt)("td",{parentName:"tr",align:null},"count"),(0,r.kt)("td",{parentName:"tr",align:null},"failed times of tcp connections between current node and other nodes")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_success_rate"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"success rate (successed/total) of tcp connection between current node and other nodes")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_min"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"mininum latency of tcp connections between current node and other nodes")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_max"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"maximum latency of tcp connections between current node and other nodes")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_avg"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"average latency of tcp connections between current node and other nodes")))),(0,r.kt)("h3",{id:"gpcnet-network-test--gpcnet-network-load-test"},(0,r.kt)("inlineCode",{parentName:"h3"},"gpcnet-network-test")," / ",(0,r.kt)("inlineCode",{parentName:"h3"},"gpcnet-network-load-test")),(0,r.kt)("h4",{id:"introduction-9"},"Introduction"),(0,r.kt)("p",null,"Distributed test, test the global network performance and congestion,\nperformed by ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/netbench/GPCNET"},"GPCNET")),(0,r.kt)("p",null,"gpcnet-network-test: Full system network tests in random and natural ring, alltoall and allreduce, at least 2 nodes"),(0,r.kt)("p",null,"gpcnet-network-load-test: Select full system network tests run with four congestors to measure network congestion or contention, at least 10 nodes"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"supporting network tests: RR Two-sided Lat (8 B), RR Get Lat (8 B), RR Two-sided BW (131072 B), RR Put BW (131072 B), RR Two-sided BW+Sync (131072 B), Nat Two-sided BW (131072 B), Multiple Allreduce (8 B), Multiple Alltoall (4096 B)"),(0,r.kt)("li",{parentName:"ul"},"supporting congestors: Alltoall (4096 B), Two-sided Incast (4096 B), Put Incast (4096 B), Get Bcast (4096 B)")),(0,r.kt)("h4",{id:"metrics-10"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Metrics"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,r.kt)("em",{parentName:"td"},"two-sided_lat"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,r.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'random ring communication pattern two-side latency' for network testing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,r.kt)("em",{parentName:"td"},"two-sided+sync_bw"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,r.kt)("td",{parentName:"tr",align:null},"fstatistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'random ring communication pattern two-side bandwidth with barrier' for network testing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/multiple",(0,r.kt)("em",{parentName:"td"},"allreduce_time"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,r.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'multiple allreduce bandwidth' for network testing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,r.kt)("em",{parentName:"td"},"get_lat"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,r.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'RR GetLat (8 B)' for network testing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,r.kt)("em",{parentName:"td"},"two-sided_bw"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,r.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'RR Two-sidedBW (131072 B)' for network testing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/nat",(0,r.kt)("em",{parentName:"td"},"two-sided_bw"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,r.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'Nat Two-sidedBW (131072 B)' for network testing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/multiple",(0,r.kt)("em",{parentName:"td"},"alltoall_bw"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,r.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'Multiple Alltoall (4096 B)' for network testing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/rr",(0,r.kt)("em",{parentName:"td"},"two-sided_lat_x"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"factor (x)"),(0,r.kt)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/rr",(0,r.kt)("em",{parentName:"td"},"two-sided+sync_bw_x"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"factor (x)"),(0,r.kt)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/multiple",(0,r.kt)("em",{parentName:"td"},"allreduce_x"),"${stat}"),(0,r.kt)("td",{parentName:"tr",align:null},"factor (x)"),(0,r.kt)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")))),(0,r.kt)("h3",{id:"ib-traffic"},(0,r.kt)("inlineCode",{parentName:"h3"},"ib-traffic")),(0,r.kt)("h4",{id:"introduction-10"},"Introduction"),(0,r.kt)("p",null,"Measure the InfiniBand performance under multi nodes' traffic pattern."),(0,r.kt)("p",null,"The traffic pattern is defined in a config file, which is pre-defined for one-to-many, many-to-one and all-to-all patterns.\nEach row in the config is one round, and all pairs of nodes in a row run ib command simultaneously."),(0,r.kt)("h4",{id:"metrics-11"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Metrics"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ib-traffic/${command}",(0,r.kt)("em",{parentName:"td"},"${line}"),"${pair}",(0,r.kt)("em",{parentName:"td"},"${server}"),"${client}_bw"),(0,r.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The max bandwidth of ib command (ib_write_bw, ib_send_bw, ib_read_bw) run between the ${pair}",(0,r.kt)("sup",null,"th")," node pair in the ${line}",(0,r.kt)("sup",null,"th")," line of the config, ${server} and ${client} are the hostname of server and client")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ib-traffic/${command}",(0,r.kt)("em",{parentName:"td"},"${line}"),"${pair}",(0,r.kt)("em",{parentName:"td"},"${server}"),"${client}_lat"),(0,r.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,r.kt)("td",{parentName:"tr",align:null},"The max latency of ib command (ib_write_lat, ib_send_lat, ib_read_lat) run between the ${pair}",(0,r.kt)("sup",null,"th")," node pair in the ${line}",(0,r.kt)("sup",null,"th")," line of the config, ${server} and ${client} are the hostname of server and client")))),(0,r.kt)("h2",{id:"computation-communication-benchmarks"},"Computation-communication Benchmarks"),(0,r.kt)("h3",{id:"computation-communication-overlap"},(0,r.kt)("inlineCode",{parentName:"h3"},"computation-communication-overlap")),(0,r.kt)("h4",{id:"introduction-11"},"Introduction"),(0,r.kt)("p",null,"Test the performance of single node when communication and computation overlap."),(0,r.kt)("h4",{id:"metrics-12"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"pytorch-computation-communication-overlap/mul_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Time of communication and mul kernel computation overlap.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"pytorch-computation-communication-overlap/matmul_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Time of communication and matmul kernel computation overlap.")))),(0,r.kt)("h4",{id:""}),(0,r.kt)("h3",{id:"sharding-matmul"},(0,r.kt)("inlineCode",{parentName:"h3"},"sharding-matmul")),(0,r.kt)("h4",{id:"introduction-12"},"Introduction"),(0,r.kt)("p",null,"Test the performance of large scale matmul operation with multiple GPUs:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"allreduce: Each GPU will calculate part of the MM calculation, and use AllReduce to merge all data into one tensor."),(0,r.kt)("li",{parentName:"ul"},"allgather: Each GPU will calculate part of the MM calculation, and use AllGather + Concat to merge all data into one tensor.")),(0,r.kt)("h4",{id:"metrics-13"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"pytorch-sharding-matmul/allreduce_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Time of sharding matmul using allreduce.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"pytorch-sharding-matmul/allgather_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Time of sharding matmul using allgather.")))),(0,r.kt)("h2",{id:"storage-benchmarks"},"Storage Benchmarks"),(0,r.kt)("h3",{id:"disk-benchmark"},(0,r.kt)("inlineCode",{parentName:"h3"},"disk-benchmark")),(0,r.kt)("h4",{id:"introduction-13"},"Introduction"),(0,r.kt)("p",null,"Measure the disk performance through ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/axboe/fio/tree/0313e938c9c8bb37d71dade239f1f5326677b079"},"FIO"),"."),(0,r.kt)("h4",{id:"metrics-14"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_bs"),(0,r.kt)("td",{parentName:"tr",align:null},"size (bytes)"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write block size.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_iops"),(0,r.kt)("td",{parentName:"tr",align:null},"IOPS"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write read IOPS.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_95.0"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write read latency in 95.0 percentile.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.0"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write read latency in 99.0 percentile.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.9"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write read latency in 99.9 percentile.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_iops"),(0,r.kt)("td",{parentName:"tr",align:null},"IOPS"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write write IOPS.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_95.0"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write write latency in 95.0 percentile.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.0"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write write latency in 99.0 percentile.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.9"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,r.kt)("td",{parentName:"tr",align:null},"Disk random read write write latency in 99.9 percentile.")))),(0,r.kt)("h1",{id:"model-benchmarks"},"Model Benchmarks"),(0,r.kt)("h2",{id:"pytorch-model-benchmarks"},"PyTorch Model Benchmarks"),(0,r.kt)("h3",{id:"gpt_models"},(0,r.kt)("inlineCode",{parentName:"h3"},"gpt_models")),(0,r.kt)("h4",{id:"introduction-14"},"Introduction"),(0,r.kt)("p",null,"Run training or inference tasks with single or half precision for GPT models,\nincluding gpt2-small, gpt2-medium, gpt2-large and gpt2-xl."),(0,r.kt)("h4",{id:"metrics-15"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpt_models/pytorch-${model_name}/fp32_train_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train step time with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpt_models/pytorch-${model_name}/fp32_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train throughput with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpt_models/pytorch-${model_name}/fp32_inference_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference step time with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpt_models/pytorch-${model_name}/fp32_inference_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference throughput with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpt_models/pytorch-${model_name}/fp16_train_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train step time with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpt_models/pytorch-${model_name}/fp16_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train throughput with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpt_models/pytorch-${model_name}/fp16_inference_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference step time with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"gpt_models/pytorch-${model_name}/fp16_inference_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference throughput with half precision.")))),(0,r.kt)("h3",{id:"bert_models"},(0,r.kt)("inlineCode",{parentName:"h3"},"bert_models")),(0,r.kt)("h4",{id:"introduction-15"},"Introduction"),(0,r.kt)("p",null,"Run training or inference tasks with single or half precision for BERT models, including bert-base and bert-large."),(0,r.kt)("h4",{id:"metrics-16"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"bert_models/pytorch-${model_name}/fp32_train_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train step time with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"bert_models/pytorch-${model_name}/fp32_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train throughput with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"bert_models/pytorch-${model_name}/fp32_inference_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference step time with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"bert_models/pytorch-${model_name}/fp32_inference_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference throughput with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"bert_models/pytorch-${model_name}/fp16_train_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train step time with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"bert_models/pytorch-${model_name}/fp16_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train throughput with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"bert_models/pytorch-${model_name}/fp16_inference_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference step time with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"bert_models/pytorch-${model_name}/fp16_inference_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference throughput with half precision.")))),(0,r.kt)("h3",{id:"lstm_models"},(0,r.kt)("inlineCode",{parentName:"h3"},"lstm_models")),(0,r.kt)("h4",{id:"introduction-16"},"Introduction"),(0,r.kt)("p",null,"Run training or inference tasks with single or half precision for one bidirectional LSTM model."),(0,r.kt)("h4",{id:"metrics-17"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"lstm_models/pytorch-lstm/fp32_train_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train step time with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"lstm_models/pytorch-lstm/fp32_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train throughput with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"lstm_models/pytorch-lstm/fp32_inference_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference step time with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"lstm_models/pytorch-lstm/fp32_inference_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference throughput with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"lstm_models/pytorch-lstm/fp16_train_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train step time with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"lstm_models/pytorch-lstm/fp16_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train throughput with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"lstm_models/pytorch-lstm/fp16_inference_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference step time with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"lstm_models/pytorch-lstm/fp16_inference_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference throughput with half precision.")))),(0,r.kt)("h3",{id:"cnn_models"},(0,r.kt)("inlineCode",{parentName:"h3"},"cnn_models")),(0,r.kt)("h4",{id:"introduction-17"},"Introduction"),(0,r.kt)("p",null,"Run training or inference tasks with single or half precision for CNN models listed in\n",(0,r.kt)("a",{parentName:"p",href:"https://pytorch.org/vision/0.8/models.html"},(0,r.kt)("inlineCode",{parentName:"a"},"torchvision.models")),", including:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"resnet: resnet18, resnet34, resnet50, resnet101, resnet152"),(0,r.kt)("li",{parentName:"ul"},"resnext: resnext50_32x4d, resnext101_32x8d"),(0,r.kt)("li",{parentName:"ul"},"wide_resnet: wide_resnet50_2, wide_resnet101_2"),(0,r.kt)("li",{parentName:"ul"},"densenet: densenet121, densenet169, densenet201, densenet161"),(0,r.kt)("li",{parentName:"ul"},"vgg: vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19"),(0,r.kt)("li",{parentName:"ul"},"mnasnet: mnasnet0_5, mnasnet0_75, mnasnet1_0, mnasnet1_3"),(0,r.kt)("li",{parentName:"ul"},"mobilenet: mobilenet_v2"),(0,r.kt)("li",{parentName:"ul"},"shufflenet: shufflenet_v2_x0_5, shufflenet_v2_x1_0, shufflenet_v2_x1_5, shufflenet_v2_x2_0"),(0,r.kt)("li",{parentName:"ul"},"squeezenet: squeezenet1_0, squeezenet1_1"),(0,r.kt)("li",{parentName:"ul"},"others: alexnet, googlenet, inception_v3")),(0,r.kt)("h4",{id:"metrics-18"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnn_models/pytorch-${model_name}/fp32_train_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train step time with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnn_models/pytorch-${model_name}/fp32_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train throughput with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnn_models/pytorch-${model_name}/fp32_inference_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference step time with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnn_models/pytorch-${model_name}/fp32_inference_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference throughput with single precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnn_models/pytorch-${model_name}/fp16_train_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train step time with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnn_models/pytorch-${model_name}/fp16_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Train throughput with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnn_models/pytorch-${model_name}/fp16_inference_step_time"),(0,r.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference step time with half precision.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnn_models/pytorch-${model_name}/fp16_inference_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"Inference throughput with half precision.")))),(0,r.kt)("hr",null),(0,r.kt)("h2",{id:"id-docker-benchmarks"},"id: docker-benchmarks"),(0,r.kt)("h1",{id:"docker-benchmarks"},"Docker Benchmarks"),(0,r.kt)("h2",{id:"rocm-onnxruntime-model-benchmarks"},"ROCm ONNXRuntime Model Benchmarks"),(0,r.kt)("h3",{id:"ort-models"},(0,r.kt)("inlineCode",{parentName:"h3"},"ort-models")),(0,r.kt)("h4",{id:"introduction-18"},"Introduction"),(0,r.kt)("p",null,"Run the rocm onnxruntime model training benchmarks packaged in docker ",(0,r.kt)("inlineCode",{parentName:"p"},"superbench/benchmark:rocm4.3.1-onnxruntime1.9.0")," which includes Bert-large, Distilbert-base, GPT-2, facebook/Bart-large and Roberta-large."),(0,r.kt)("h4",{id:"metrics-19"},"Metrics"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Unit"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/bert_large_uncased_ngpu_1_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of bert large uncased model on 1 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/bert_large_uncased_ngpu_8_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of bert large uncased model on 8 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/distilbert_base_uncased_ngpu_1_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of distilbert base uncased model on 1 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/distilbert_base_uncased_ngpu_8_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of distilbert base uncased model on 8 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/gpt2_ngpu_1_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of gpt2 model on 1 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/gpt2_ngpu_8_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of gpt2 model on 8 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/facebook_bart_large_ngpu_1_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of facebook bart large model on 1 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/facebook_bart_large_ngpu_8_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of facebook bart large model on 8 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/roberta_large_ngpu_1_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of roberta large model on 1 GPU.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"onnxruntime-ort-models/roberta_large_ngpu_8_train_throughput"),(0,r.kt)("td",{parentName:"tr",align:null},"throughput (samples/s)"),(0,r.kt)("td",{parentName:"tr",align:null},"The throughput of roberta large model on 8 GPU.")))))}u.isMDXComponent=!0}}]);