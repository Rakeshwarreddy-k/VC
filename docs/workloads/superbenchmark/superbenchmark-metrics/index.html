<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-workloads/superbenchmark/superbenchmark-metrics">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">SuperBenchmark Workload Metrics | Virtual Client</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://microsoft.github.io/VirtualClient/docs/workloads/superbenchmark/superbenchmark-metrics/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="performance, benchmarking, automation, framework, microsoft"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="SuperBenchmark Workload Metrics | Virtual Client"><meta data-rh="true" name="description" content="The following document illustrates the type of results that are emitted by the SuperBenchmark workload and captured by the"><meta data-rh="true" property="og:description" content="The following document illustrates the type of results that are emitted by the SuperBenchmark workload and captured by the"><link data-rh="true" rel="icon" href="/VirtualClient/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://microsoft.github.io/VirtualClient/docs/workloads/superbenchmark/superbenchmark-metrics/"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/VirtualClient/docs/workloads/superbenchmark/superbenchmark-metrics/" hreflang="en"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/VirtualClient/docs/workloads/superbenchmark/superbenchmark-metrics/" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/VirtualClient/blog/rss.xml" title="Virtual Client RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/VirtualClient/blog/atom.xml" title="Virtual Client Atom Feed"><link rel="stylesheet" href="/VirtualClient/assets/css/styles.d67ab541.css">
<link rel="preload" href="/VirtualClient/assets/js/runtime~main.6a9313fa.js" as="script">
<link rel="preload" href="/VirtualClient/assets/js/main.8bcee928.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">⭐️ If you like VirtualClient, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/VirtualClient">GitHub</a> ⭐️</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/VirtualClient/"><div class="navbar__logo"><img src="/VirtualClient/img/vc-logo.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/VirtualClient/img/vc-logo.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Virtual Client</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/VirtualClient/docs/overview/">Documentation</a><a class="navbar__item navbar__link" href="/VirtualClient/blog/">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/VirtualClient" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/VirtualClient/docs/overview/">Overview</a><button aria-label="Toggle the collapsible sidebar category &#x27;Overview&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/VirtualClient/docs/category/guides/">Guides</a><button aria-label="Toggle the collapsible sidebar category &#x27;Guides&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/VirtualClient/docs/category/dependencies/">Dependencies</a><button aria-label="Toggle the collapsible sidebar category &#x27;Dependencies&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/VirtualClient/docs/category/workloads/">Workloads</a><button aria-label="Toggle the collapsible sidebar category &#x27;Workloads&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/memcached/">MEMCACHED Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;MEMCACHED Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/openssl/">OpenSSL Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;OpenSSL Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/redis/">REDIS Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;REDIS Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/stress-ng/">StressNg</a><button aria-label="Toggle the collapsible sidebar category &#x27;StressNg&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/aspnetbench/">AspNetBenchmark</a><button aria-label="Toggle the collapsible sidebar category &#x27;AspNetBenchmark&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/compression/7zip/">compression</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/coremark/">CoreMark</a><button aria-label="Toggle the collapsible sidebar category &#x27;CoreMark&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/deathstarbench/">DeathStarBench</a><button aria-label="Toggle the collapsible sidebar category &#x27;DeathStarBench&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/diskspd/">DiskSpd</a><button aria-label="Toggle the collapsible sidebar category &#x27;DiskSpd&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/fio/">FIO (Flexible IO Tester) Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;FIO (Flexible IO Tester) Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/graph500/">Graph500</a><button aria-label="Toggle the collapsible sidebar category &#x27;Graph500&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/hpcg/">HPCG</a><button aria-label="Toggle the collapsible sidebar category &#x27;HPCG&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/lapack/">LAPACK Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;LAPACK Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/lmbench/">LMbench Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;LMbench Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/mlperf/">MLPerf Workload Suite</a><button aria-label="Toggle the collapsible sidebar category &#x27;MLPerf Workload Suite&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/nasparallel/">NAS Parallel Benchmarks Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;NAS Parallel Benchmarks Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/network/network-suite-metrics/">network</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/network-ping/">Network Ping/ICMP</a><button aria-label="Toggle the collapsible sidebar category &#x27;Network Ping/ICMP&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/openfoam/">OpenFOAM</a><button aria-label="Toggle the collapsible sidebar category &#x27;OpenFOAM&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/prime95/">Prime95 Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;Prime95 Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/VirtualClient/docs/workloads/specjvm/">SPECjvm Workload</a><button aria-label="Toggle the collapsible sidebar category &#x27;SPECjvm Workload&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/VirtualClient/docs/workloads/superbenchmark/">SuperBenchmark</a><button aria-label="Toggle the collapsible sidebar category &#x27;SuperBenchmark&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/VirtualClient/docs/workloads/superbenchmark/superbenchmark-metrics/">SuperBenchmark Workload Metrics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/VirtualClient/docs/workloads/superbenchmark/superbenchmark-profiles/">SuperBenchmark Workload Profiles</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/VirtualClient/docs/category/monitors/">Monitors</a><button aria-label="Toggle the collapsible sidebar category &#x27;Monitors&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/VirtualClient/docs/category/developing/">Developing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Developing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/VirtualClient/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/VirtualClient/docs/category/workloads/"><span itemprop="name">Workloads</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/VirtualClient/docs/workloads/superbenchmark/"><span itemprop="name">SuperBenchmark</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">SuperBenchmark Workload Metrics</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>SuperBenchmark Workload Metrics</h1><p>The following document illustrates the type of results that are emitted by the SuperBenchmark workload and captured by the
Virtual Client for net impact analysis.</p><ul><li><a href="https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks" target="_blank" rel="noopener noreferrer">SuperBenchmark MicroBenchmarks</a></li><li><a href="https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/model-benchmarks" target="_blank" rel="noopener noreferrer">SuperBenchmark ModelBenchmarks</a>  </li><li><a href="https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/docker-benchmarks" target="_blank" rel="noopener noreferrer">SuperBenchmark DockerBenchmarks</a>  </li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="workload-specific-metrics">Workload-Specific Metrics<a class="hash-link" href="#workload-specific-metrics" title="Direct link to heading">​</a></h3><p>The following metrics are captured during the operations of the SuperBenchmark workload.</p><h1>Micro Benchmarks</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="computation-benchmarks">Computation Benchmarks<a class="hash-link" href="#computation-benchmarks" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="kernel-launch"><code>kernel-launch</code><a class="hash-link" href="#kernel-launch" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h4><p>Measure GPU kernel launch latency,
which is defined as the time range from the beginning of the launch API call to the beginning of the kernel execution.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics">Metrics<a class="hash-link" href="#metrics" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>kernel-launch/event_time</td><td>time (ms)</td><td>Launch latency measured in GPU time.</td></tr><tr><td>kernel-launch/wall_time</td><td>time (ms)</td><td>Launch latency measured in CPU time.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gemm-flops"><code>gemm-flops</code><a class="hash-link" href="#gemm-flops" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-1">Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">​</a></h4><p>Measure the GPU GEMM FLOPS for different float and int data types, with or without Tensor Core (XDLOPS),
performed by NVIDIA <a href="https://github.com/NVIDIA/cutlass/tree/ccb697bac77fcc898e9c897b2c90aa5b60ac72fb" target="_blank" rel="noopener noreferrer">cutlass</a>
or AMD <a href="https://github.com/ROCmSoftwarePlatform/rocBLAS/tree/develop/clients/benchmarks" target="_blank" rel="noopener noreferrer">rocblas-bench</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-1">Metrics<a class="hash-link" href="#metrics-1" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>gemm-flops/fp64_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float64 peak FLOPS.</td></tr><tr><td>gemm-flops/fp32_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float32 peak FLOPS.</td></tr><tr><td>gemm-flops/fp16_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float16 peak FLOPS.</td></tr><tr><td>gemm-flops/fp64_tc_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float64 peak FLOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/tf32_tc_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM tensor-float32 peak FLOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/fp16_tc_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float16 peak FLOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/bf16_tc_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM bfloat16 peak FLOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/int8_tc_iops</td><td>IOPS (GIOPS)</td><td>GEMM int8 peak IOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/int4_tc_iops</td><td>IOPS (GIOPS)</td><td>GEMM int4 peak IOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/fp32_xdlops_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM tensor-float32 peak FLOPS with AMD XDLOPS.</td></tr><tr><td>gemm-flops/fp16_xdlops_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float16 peak FLOPS with AMD XDLOPS.</td></tr><tr><td>gemm-flops/bf16_xdlops_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM bfloat16 peak FLOPS with AMD XDLOPS.</td></tr><tr><td>gemm-flops/int8_xdlops_iops</td><td>IOPS (GIOPS)</td><td>GEMM int8 peak IOPS with AMD XDLOPS.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="matmul"><code>matmul</code><a class="hash-link" href="#matmul" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-2">Introduction<a class="hash-link" href="#introduction-2" title="Direct link to heading">​</a></h4><p>Large scale matmul operation using <code>torch.matmul</code> with one GPU.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-2">Metrics<a class="hash-link" href="#metrics-2" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>pytorch-matmul/nosharding_time</td><td>time (ms)</td><td>Time of pure matmul operation.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cublas-function"><code>cublas-function</code><a class="hash-link" href="#cublas-function" title="Direct link to heading">​</a></h3><p>TODO</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cudnn-function"><code>cudnn-function</code><a class="hash-link" href="#cudnn-function" title="Direct link to heading">​</a></h3><p>TODO</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tensorrt-inference"><code>tensorrt-inference</code><a class="hash-link" href="#tensorrt-inference" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-3">Introduction<a class="hash-link" href="#introduction-3" title="Direct link to heading">​</a></h4><p>Inference PyTorch/ONNX models on NVIDIA GPUs with <a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="noopener noreferrer">TensorRT</a>.
Currently the following models are supported:</p><blockquote><p>alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,
mnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,
resnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,
squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19</p></blockquote><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-3">Metrics<a class="hash-link" href="#metrics-3" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>tensorrt-inference/${model}_gpu_time_mean</td><td>time (ms)</td><td>The mean GPU latency to execute the kernels for a query.</td></tr><tr><td>tensorrt-inference/${model}_gpu_time_99</td><td>time (ms)</td><td>The 99th percentile GPU latency to execute the kernels for a query.</td></tr><tr><td>tensorrt-inference/${model}_host_time_mean</td><td>time (ms)</td><td>The mean H2D, GPU, and D2H latency to execute the kernels for a query.</td></tr><tr><td>tensorrt-inference/${model}_host_time_99</td><td>time (ms)</td><td>The 99th percentile H2D, GPU, and D2H latency to execute the kernels for a query.</td></tr><tr><td>tensorrt-inference/${model}_end_to_end_time_mean</td><td>time (ms)</td><td>The mean duration from when the H2D of a query is called to when the D2H of the same query is completed.</td></tr><tr><td>tensorrt-inference/${model}_end_to_end_time_99</td><td>time (ms)</td><td>The P99 duration from when the H2D of a query is called to when the D2H of the same query is completed.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ort-inference"><code>ort-inference</code><a class="hash-link" href="#ort-inference" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-4">Introduction<a class="hash-link" href="#introduction-4" title="Direct link to heading">​</a></h4><p>Inference performance of the torchvision models using ONNXRuntime. Currently the following models are supported:</p><blockquote><p>alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,
mnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,
resnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,
squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19</p></blockquote><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-4">Metrics<a class="hash-link" href="#metrics-4" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>ort-inference/{precision}_{model}_time</td><td>time (ms)</td><td>The mean latency to execute one batch of inference.</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="communication-benchmarks">Communication Benchmarks<a class="hash-link" href="#communication-benchmarks" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="mem-bw"><code>mem-bw</code><a class="hash-link" href="#mem-bw" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-5">Introduction<a class="hash-link" href="#introduction-5" title="Direct link to heading">​</a></h4><p>Measure the memory copy bandwidth across PCI-e and memory copy bandwidth between GPUs,
performed by <a href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/bandwidthTest" target="_blank" rel="noopener noreferrer">NVIDIA</a>
or <a href="https://github.com/ROCm-Developer-Tools/HIP/tree/master/samples/1_Utils/hipBusBandwidth" target="_blank" rel="noopener noreferrer">AMD</a> bandwidth test tool.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-5">Metrics<a class="hash-link" href="#metrics-5" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>mem-bw/h2d_bw</td><td>bandwidth (GB/s)</td><td>Host to device copy bandwidth.</td></tr><tr><td>mem-bw/d2h_bw</td><td>bandwidth (GB/s)</td><td>Device to host copy bandwidth.</td></tr><tr><td>mem-bw/d2d_bw</td><td>bandwidth (GB/s)</td><td>Device to device copy bandwidth.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-copy-bw"><code>gpu-copy-bw</code><a class="hash-link" href="#gpu-copy-bw" title="Direct link to heading">​</a></h3><p>Measure the memory copy bandwidth performed by GPU SM/DMA engine, including device-to-host, host-to-device and device-to-device.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-6">Metrics<a class="hash-link" href="#metrics-6" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>cpu<!-- -->_<!-- -->to<!-- -->_<!-- -->gpu<!-- -->[0-9]<!-- -->+<!-- -->_<!-- -->by<!-- -->_<!-- -->gpu<!-- -->[0-9]<!-- -->+<!-- -->_<!-- -->using<!-- -->_<!-- -->(sm<!-- -->|<!-- -->dma)<!-- -->_<!-- -->under_numa<!-- -->[0-9]<!-- -->+_bw</td><td>bandwidth (GB/s)</td><td>The bandwidth reading from all NUMA nodes&#x27; host memory using DMA engine or GPU SM by all GPUs.</td></tr><tr><td>gpu<!-- -->[0-9]<!-- -->+<!-- -->_<!-- -->to<!-- -->_<!-- -->cpu<!-- -->_<!-- -->by<!-- -->_<!-- -->gpu<!-- -->[0-9]<!-- -->+<!-- -->_<!-- -->using<!-- -->_<!-- -->(sm<!-- -->|<!-- -->dma)<!-- -->_<!-- -->under_numa<!-- -->[0-9]<!-- -->+_bw</td><td>bandwidth (GB/s)</td><td>The bandwidth writing to all NUMA nodes&#x27; host memory using DMA engine or GPU SM by all GPUs.</td></tr><tr><td>gpu<!-- -->[0-9]<!-- -->+<!-- -->_<!-- -->to_gpu<!-- -->[0-9]<!-- -->+<!-- -->_<!-- -->by<!-- -->_<!-- -->gpu<!-- -->[0-9]<!-- -->+<!-- -->_<!-- -->using<!-- -->_<!-- -->(sm<!-- -->|<!-- -->dma)<!-- -->_<!-- -->under_numa<!-- -->[0-9]<!-- -->+_bw</td><td>bandwidth (GB/s)</td><td>The bandwidth reading from  or writing to all GPUs using DMA engine or GPU SM by all GPUs with peer communication enabled.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ib-loopback"><code>ib-loopback</code><a class="hash-link" href="#ib-loopback" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-6">Introduction<a class="hash-link" href="#introduction-6" title="Direct link to heading">​</a></h4><p>Measure the InfiniBand loopback verbs bandwidth, performed by
<a href="https://github.com/linux-rdma/perftest/tree/7504ce48ac396a02f4d00de359257b2cb8458f06" target="_blank" rel="noopener noreferrer">OFED performance tests</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-7">Metrics<a class="hash-link" href="#metrics-7" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>ib-loopback/ib<em>write</em>${msg_size}_ib<!-- -->[0-9]<!-- -->_bw</td><td>bandwidth (GB/s)</td><td>InfiniBand loopback write bandwidth with given message size.</td></tr><tr><td>ib-loopback/ib<em>read</em>${msg_size}_ib<!-- -->[0-9]<!-- -->_bw</td><td>bandwidth (GB/s)</td><td>InfiniBand loopback read bandwidth with given message size.</td></tr><tr><td>ib-loopback/ib<em>send</em>${msg_size}_ib<!-- -->[0-9]<!-- -->_bw</td><td>bandwidth (GB/s)</td><td>InfiniBand loopback send bandwidth with given message size.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="nccl-bw--rccl-bw"><code>nccl-bw</code> / <code>rccl-bw</code><a class="hash-link" href="#nccl-bw--rccl-bw" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-7">Introduction<a class="hash-link" href="#introduction-7" title="Direct link to heading">​</a></h4><p>Measure the performance of NCCL/RCCL operations,
performed by <a href="https://github.com/NVIDIA/nccl-tests/tree/44df0bf010dcc95e840ca0fb7466c67cff3f1f0f" target="_blank" rel="noopener noreferrer">nccl-tests</a>
or <a href="https://github.com/ROCmSoftwarePlatform/rccl-tests/tree/dc1ad4853d7ec738387d42a75a58a98d7af00c7b" target="_blank" rel="noopener noreferrer">rccl-tests</a>.
Support the following operations currently: allreduce, allgather, broadcast, reduce, reducescatter, alltoall.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-8">Metrics<a class="hash-link" href="#metrics-8" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>nccl-bw/${operation}_${msg_size}_time</td><td>time (us)</td><td>NCCL operation lantency with given message size.</td></tr><tr><td>nccl-bw/${operation}_${msg_size}_algbw</td><td>bandwidth (GB/s)</td><td>NCCL operation algorithm bandwidth with given message size.</td></tr><tr><td>nccl-bw/${operation}_${msg_size}_busbw</td><td>bandwidth (GB/s)</td><td>NCCL operation bus bandwidth with given message size.</td></tr><tr><td>rccl-bw/${operation}_${msg_size}_time</td><td>time (us)</td><td>RCCL operation lantency with given message size.</td></tr><tr><td>rccl-bw/${operation}_${msg_size}_algbw</td><td>bandwidth (GB/s)</td><td>RCCL operation algorithm bandwidth with given message size.</td></tr><tr><td>rccl-bw/${operation}_${msg_size}_busbw</td><td>bandwidth (GB/s)</td><td>RCCL operation bus bandwidth with given message size.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tcp-connectivity"><code>tcp-connectivity</code><a class="hash-link" href="#tcp-connectivity" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-8">Introduction<a class="hash-link" href="#introduction-8" title="Direct link to heading">​</a></h4><p>Test the TCP connectivity between current node and nodes in the hostfile,
performed by <a href="https://github.com/zhengxiaowai/tcping" target="_blank" rel="noopener noreferrer">tcping</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-9">Metrics<a class="hash-link" href="#metrics-9" title="Direct link to heading">​</a></h4><table><thead><tr><th>Metrics</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>tcp-connectivity/${hostname/ip}_successed_count</td><td>count</td><td>successed times of tcp connections between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_failed_count</td><td>count</td><td>failed times of tcp connections between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_success_rate</td><td></td><td>success rate (successed/total) of tcp connection between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_time_min</td><td>time (ms)</td><td>mininum latency of tcp connections between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_time_max</td><td>time (ms)</td><td>maximum latency of tcp connections between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_time_avg</td><td>time (ms)</td><td>average latency of tcp connections between current node and other nodes</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpcnet-network-test--gpcnet-network-load-test"><code>gpcnet-network-test</code> / <code>gpcnet-network-load-test</code><a class="hash-link" href="#gpcnet-network-test--gpcnet-network-load-test" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-9">Introduction<a class="hash-link" href="#introduction-9" title="Direct link to heading">​</a></h4><p>Distributed test, test the global network performance and congestion,
performed by <a href="https://github.com/netbench/GPCNET" target="_blank" rel="noopener noreferrer">GPCNET</a></p><p>gpcnet-network-test: Full system network tests in random and natural ring, alltoall and allreduce, at least 2 nodes</p><p>gpcnet-network-load-test: Select full system network tests run with four congestors to measure network congestion or contention, at least 10 nodes</p><ul><li>supporting network tests: RR Two-sided Lat (8 B), RR Get Lat (8 B), RR Two-sided BW (131072 B), RR Put BW (131072 B), RR Two-sided BW+Sync (131072 B), Nat Two-sided BW (131072 B), Multiple Allreduce (8 B), Multiple Alltoall (4096 B)</li><li>supporting congestors: Alltoall (4096 B), Two-sided Incast (4096 B), Put Incast (4096 B), Get Bcast (4096 B)</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-10">Metrics<a class="hash-link" href="#metrics-10" title="Direct link to heading">​</a></h4><table><thead><tr><th>Metrics</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>gpcnet-network-test/rr<em>two-sided_lat</em>${stat}</td><td>time (us)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;random ring communication pattern two-side latency&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/rr<em>two-sided+sync_bw</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>fstatistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;random ring communication pattern two-side bandwidth with barrier&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/multiple<em>allreduce_time</em>${stat}</td><td>time (us)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;multiple allreduce bandwidth&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/rr<em>get_lat</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;RR GetLat (8 B)&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/rr<em>two-sided_bw</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;RR Two-sidedBW (131072 B)&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/nat<em>two-sided_bw</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;Nat Two-sidedBW (131072 B)&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/multiple<em>alltoall_bw</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;Multiple Alltoall (4096 B)&#x27; for network testing</td></tr><tr><td>gpcnet-network-load-test/rr<em>two-sided_lat_x</em>${stat}</td><td>factor (x)</td><td>summary about congestion impact factor of the network test algorithm</td></tr><tr><td>gpcnet-network-load-test/rr<em>two-sided+sync_bw_x</em>${stat}</td><td>factor (x)</td><td>summary about congestion impact factor of the network test algorithm</td></tr><tr><td>gpcnet-network-load-test/multiple<em>allreduce_x</em>${stat}</td><td>factor (x)</td><td>summary about congestion impact factor of the network test algorithm</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ib-traffic"><code>ib-traffic</code><a class="hash-link" href="#ib-traffic" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-10">Introduction<a class="hash-link" href="#introduction-10" title="Direct link to heading">​</a></h4><p>Measure the InfiniBand performance under multi nodes&#x27; traffic pattern.</p><p>The traffic pattern is defined in a config file, which is pre-defined for one-to-many, many-to-one and all-to-all patterns.
Each row in the config is one round, and all pairs of nodes in a row run ib command simultaneously.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-11">Metrics<a class="hash-link" href="#metrics-11" title="Direct link to heading">​</a></h4><table><thead><tr><th>Metrics</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>ib-traffic/${command}<em>${line}</em>${pair}<em>${server}</em>${client}_bw</td><td>bandwidth (GB/s)</td><td>The max bandwidth of ib command (ib_write_bw, ib_send_bw, ib_read_bw) run between the ${pair}<sup>th</sup> node pair in the ${line}<sup>th</sup> line of the config, ${server} and ${client} are the hostname of server and client</td></tr><tr><td>ib-traffic/${command}<em>${line}</em>${pair}<em>${server}</em>${client}_lat</td><td>time (us)</td><td>The max latency of ib command (ib_write_lat, ib_send_lat, ib_read_lat) run between the ${pair}<sup>th</sup> node pair in the ${line}<sup>th</sup> line of the config, ${server} and ${client} are the hostname of server and client</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="computation-communication-benchmarks">Computation-communication Benchmarks<a class="hash-link" href="#computation-communication-benchmarks" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="computation-communication-overlap"><code>computation-communication-overlap</code><a class="hash-link" href="#computation-communication-overlap" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-11">Introduction<a class="hash-link" href="#introduction-11" title="Direct link to heading">​</a></h4><p>Test the performance of single node when communication and computation overlap.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-12">Metrics<a class="hash-link" href="#metrics-12" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>pytorch-computation-communication-overlap/mul_time</td><td>time (ms)</td><td>Time of communication and mul kernel computation overlap.</td></tr><tr><td>pytorch-computation-communication-overlap/matmul_time</td><td>time (ms)</td><td>Time of communication and matmul kernel computation overlap.</td></tr></tbody></table><h4></h4><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sharding-matmul"><code>sharding-matmul</code><a class="hash-link" href="#sharding-matmul" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-12">Introduction<a class="hash-link" href="#introduction-12" title="Direct link to heading">​</a></h4><p>Test the performance of large scale matmul operation with multiple GPUs:</p><ul><li>allreduce: Each GPU will calculate part of the MM calculation, and use AllReduce to merge all data into one tensor.</li><li>allgather: Each GPU will calculate part of the MM calculation, and use AllGather + Concat to merge all data into one tensor.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-13">Metrics<a class="hash-link" href="#metrics-13" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>pytorch-sharding-matmul/allreduce_time</td><td>time (ms)</td><td>Time of sharding matmul using allreduce.</td></tr><tr><td>pytorch-sharding-matmul/allgather_time</td><td>time (ms)</td><td>Time of sharding matmul using allgather.</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="storage-benchmarks">Storage Benchmarks<a class="hash-link" href="#storage-benchmarks" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="disk-benchmark"><code>disk-benchmark</code><a class="hash-link" href="#disk-benchmark" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-13">Introduction<a class="hash-link" href="#introduction-13" title="Direct link to heading">​</a></h4><p>Measure the disk performance through <a href="https://github.com/axboe/fio/tree/0313e938c9c8bb37d71dade239f1f5326677b079" target="_blank" rel="noopener noreferrer">FIO</a>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-14">Metrics<a class="hash-link" href="#metrics-14" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>disk-benchmark/${disk_name}_rand_read_write_bs</td><td>size (bytes)</td><td>Disk random read write block size.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_read_iops</td><td>IOPS</td><td>Disk random read write read IOPS.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_95.0</td><td>time (ns)</td><td>Disk random read write read latency in 95.0 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.0</td><td>time (ns)</td><td>Disk random read write read latency in 99.0 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.9</td><td>time (ns)</td><td>Disk random read write read latency in 99.9 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_write_iops</td><td>IOPS</td><td>Disk random read write write IOPS.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_95.0</td><td>time (ns)</td><td>Disk random read write write latency in 95.0 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.0</td><td>time (ns)</td><td>Disk random read write write latency in 99.0 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.9</td><td>time (ns)</td><td>Disk random read write write latency in 99.9 percentile.</td></tr></tbody></table><h1>Model Benchmarks</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pytorch-model-benchmarks">PyTorch Model Benchmarks<a class="hash-link" href="#pytorch-model-benchmarks" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpt_models"><code>gpt_models</code><a class="hash-link" href="#gpt_models" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-14">Introduction<a class="hash-link" href="#introduction-14" title="Direct link to heading">​</a></h4><p>Run training or inference tasks with single or half precision for GPT models,
including gpt2-small, gpt2-medium, gpt2-large and gpt2-xl.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-15">Metrics<a class="hash-link" href="#metrics-15" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>gpt_models/pytorch-${model_name}/fp32_train_step_time</td><td>time (ms)</td><td>Train step time with single precision.</td></tr><tr><td>gpt_models/pytorch-${model_name}/fp32_train_throughput</td><td>throughput (samples/s)</td><td>Train throughput with single precision.</td></tr><tr><td>gpt_models/pytorch-${model_name}/fp32_inference_step_time</td><td>time (ms)</td><td>Inference step time with single precision.</td></tr><tr><td>gpt_models/pytorch-${model_name}/fp32_inference_throughput</td><td>throughput (samples/s)</td><td>Inference throughput with single precision.</td></tr><tr><td>gpt_models/pytorch-${model_name}/fp16_train_step_time</td><td>time (ms)</td><td>Train step time with half precision.</td></tr><tr><td>gpt_models/pytorch-${model_name}/fp16_train_throughput</td><td>throughput (samples/s)</td><td>Train throughput with half precision.</td></tr><tr><td>gpt_models/pytorch-${model_name}/fp16_inference_step_time</td><td>time (ms)</td><td>Inference step time with half precision.</td></tr><tr><td>gpt_models/pytorch-${model_name}/fp16_inference_throughput</td><td>throughput (samples/s)</td><td>Inference throughput with half precision.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bert_models"><code>bert_models</code><a class="hash-link" href="#bert_models" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-15">Introduction<a class="hash-link" href="#introduction-15" title="Direct link to heading">​</a></h4><p>Run training or inference tasks with single or half precision for BERT models, including bert-base and bert-large.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-16">Metrics<a class="hash-link" href="#metrics-16" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>bert_models/pytorch-${model_name}/fp32_train_step_time</td><td>time (ms)</td><td>Train step time with single precision.</td></tr><tr><td>bert_models/pytorch-${model_name}/fp32_train_throughput</td><td>throughput (samples/s)</td><td>Train throughput with single precision.</td></tr><tr><td>bert_models/pytorch-${model_name}/fp32_inference_step_time</td><td>time (ms)</td><td>Inference step time with single precision.</td></tr><tr><td>bert_models/pytorch-${model_name}/fp32_inference_throughput</td><td>throughput (samples/s)</td><td>Inference throughput with single precision.</td></tr><tr><td>bert_models/pytorch-${model_name}/fp16_train_step_time</td><td>time (ms)</td><td>Train step time with half precision.</td></tr><tr><td>bert_models/pytorch-${model_name}/fp16_train_throughput</td><td>throughput (samples/s)</td><td>Train throughput with half precision.</td></tr><tr><td>bert_models/pytorch-${model_name}/fp16_inference_step_time</td><td>time (ms)</td><td>Inference step time with half precision.</td></tr><tr><td>bert_models/pytorch-${model_name}/fp16_inference_throughput</td><td>throughput (samples/s)</td><td>Inference throughput with half precision.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lstm_models"><code>lstm_models</code><a class="hash-link" href="#lstm_models" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-16">Introduction<a class="hash-link" href="#introduction-16" title="Direct link to heading">​</a></h4><p>Run training or inference tasks with single or half precision for one bidirectional LSTM model.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-17">Metrics<a class="hash-link" href="#metrics-17" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>lstm_models/pytorch-lstm/fp32_train_step_time</td><td>time (ms)</td><td>Train step time with single precision.</td></tr><tr><td>lstm_models/pytorch-lstm/fp32_train_throughput</td><td>throughput (samples/s)</td><td>Train throughput with single precision.</td></tr><tr><td>lstm_models/pytorch-lstm/fp32_inference_step_time</td><td>time (ms)</td><td>Inference step time with single precision.</td></tr><tr><td>lstm_models/pytorch-lstm/fp32_inference_throughput</td><td>throughput (samples/s)</td><td>Inference throughput with single precision.</td></tr><tr><td>lstm_models/pytorch-lstm/fp16_train_step_time</td><td>time (ms)</td><td>Train step time with half precision.</td></tr><tr><td>lstm_models/pytorch-lstm/fp16_train_throughput</td><td>throughput (samples/s)</td><td>Train throughput with half precision.</td></tr><tr><td>lstm_models/pytorch-lstm/fp16_inference_step_time</td><td>time (ms)</td><td>Inference step time with half precision.</td></tr><tr><td>lstm_models/pytorch-lstm/fp16_inference_throughput</td><td>throughput (samples/s)</td><td>Inference throughput with half precision.</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cnn_models"><code>cnn_models</code><a class="hash-link" href="#cnn_models" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-17">Introduction<a class="hash-link" href="#introduction-17" title="Direct link to heading">​</a></h4><p>Run training or inference tasks with single or half precision for CNN models listed in
<a href="https://pytorch.org/vision/0.8/models.html" target="_blank" rel="noopener noreferrer"><code>torchvision.models</code></a>, including:</p><ul><li>resnet: resnet18, resnet34, resnet50, resnet101, resnet152</li><li>resnext: resnext50_32x4d, resnext101_32x8d</li><li>wide_resnet: wide_resnet50_2, wide_resnet101_2</li><li>densenet: densenet121, densenet169, densenet201, densenet161</li><li>vgg: vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19</li><li>mnasnet: mnasnet0_5, mnasnet0_75, mnasnet1_0, mnasnet1_3</li><li>mobilenet: mobilenet_v2</li><li>shufflenet: shufflenet_v2_x0_5, shufflenet_v2_x1_0, shufflenet_v2_x1_5, shufflenet_v2_x2_0</li><li>squeezenet: squeezenet1_0, squeezenet1_1</li><li>others: alexnet, googlenet, inception_v3</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-18">Metrics<a class="hash-link" href="#metrics-18" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>cnn_models/pytorch-${model_name}/fp32_train_step_time</td><td>time (ms)</td><td>Train step time with single precision.</td></tr><tr><td>cnn_models/pytorch-${model_name}/fp32_train_throughput</td><td>throughput (samples/s)</td><td>Train throughput with single precision.</td></tr><tr><td>cnn_models/pytorch-${model_name}/fp32_inference_step_time</td><td>time (ms)</td><td>Inference step time with single precision.</td></tr><tr><td>cnn_models/pytorch-${model_name}/fp32_inference_throughput</td><td>throughput (samples/s)</td><td>Inference throughput with single precision.</td></tr><tr><td>cnn_models/pytorch-${model_name}/fp16_train_step_time</td><td>time (ms)</td><td>Train step time with half precision.</td></tr><tr><td>cnn_models/pytorch-${model_name}/fp16_train_throughput</td><td>throughput (samples/s)</td><td>Train throughput with half precision.</td></tr><tr><td>cnn_models/pytorch-${model_name}/fp16_inference_step_time</td><td>time (ms)</td><td>Inference step time with half precision.</td></tr><tr><td>cnn_models/pytorch-${model_name}/fp16_inference_throughput</td><td>throughput (samples/s)</td><td>Inference throughput with half precision.</td></tr></tbody></table><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="id-docker-benchmarks">id: docker-benchmarks<a class="hash-link" href="#id-docker-benchmarks" title="Direct link to heading">​</a></h2><h1>Docker Benchmarks</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="rocm-onnxruntime-model-benchmarks">ROCm ONNXRuntime Model Benchmarks<a class="hash-link" href="#rocm-onnxruntime-model-benchmarks" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ort-models"><code>ort-models</code><a class="hash-link" href="#ort-models" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-18">Introduction<a class="hash-link" href="#introduction-18" title="Direct link to heading">​</a></h4><p>Run the rocm onnxruntime model training benchmarks packaged in docker <code>superbench/benchmark:rocm4.3.1-onnxruntime1.9.0</code> which includes Bert-large, Distilbert-base, GPT-2, facebook/Bart-large and Roberta-large.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-19">Metrics<a class="hash-link" href="#metrics-19" title="Direct link to heading">​</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>onnxruntime-ort-models/bert_large_uncased_ngpu_1_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of bert large uncased model on 1 GPU.</td></tr><tr><td>onnxruntime-ort-models/bert_large_uncased_ngpu_8_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of bert large uncased model on 8 GPU.</td></tr><tr><td>onnxruntime-ort-models/distilbert_base_uncased_ngpu_1_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of distilbert base uncased model on 1 GPU.</td></tr><tr><td>onnxruntime-ort-models/distilbert_base_uncased_ngpu_8_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of distilbert base uncased model on 8 GPU.</td></tr><tr><td>onnxruntime-ort-models/gpt2_ngpu_1_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of gpt2 model on 1 GPU.</td></tr><tr><td>onnxruntime-ort-models/gpt2_ngpu_8_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of gpt2 model on 8 GPU.</td></tr><tr><td>onnxruntime-ort-models/facebook_bart_large_ngpu_1_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of facebook bart large model on 1 GPU.</td></tr><tr><td>onnxruntime-ort-models/facebook_bart_large_ngpu_8_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of facebook bart large model on 8 GPU.</td></tr><tr><td>onnxruntime-ort-models/roberta_large_ngpu_1_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of roberta large model on 1 GPU.</td></tr><tr><td>onnxruntime-ort-models/roberta_large_ngpu_8_train_throughput</td><td>throughput (samples/s)</td><td>The throughput of roberta large model on 8 GPU.</td></tr></tbody></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/microsoft/VirtualClient/edit/main/website/docs/workloads/superbenchmark/superbenchmark-metrics.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/VirtualClient/docs/workloads/superbenchmark/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">SuperBenchmark</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/VirtualClient/docs/workloads/superbenchmark/superbenchmark-profiles/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">SuperBenchmark Workload Profiles</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#workload-specific-metrics" class="table-of-contents__link toc-highlight">Workload-Specific Metrics</a></li><li><a href="#computation-benchmarks" class="table-of-contents__link toc-highlight">Computation Benchmarks</a><ul><li><a href="#kernel-launch" class="table-of-contents__link toc-highlight"><code>kernel-launch</code></a></li><li><a href="#gemm-flops" class="table-of-contents__link toc-highlight"><code>gemm-flops</code></a></li><li><a href="#matmul" class="table-of-contents__link toc-highlight"><code>matmul</code></a></li><li><a href="#cublas-function" class="table-of-contents__link toc-highlight"><code>cublas-function</code></a></li><li><a href="#cudnn-function" class="table-of-contents__link toc-highlight"><code>cudnn-function</code></a></li><li><a href="#tensorrt-inference" class="table-of-contents__link toc-highlight"><code>tensorrt-inference</code></a></li><li><a href="#ort-inference" class="table-of-contents__link toc-highlight"><code>ort-inference</code></a></li></ul></li><li><a href="#communication-benchmarks" class="table-of-contents__link toc-highlight">Communication Benchmarks</a><ul><li><a href="#mem-bw" class="table-of-contents__link toc-highlight"><code>mem-bw</code></a></li><li><a href="#gpu-copy-bw" class="table-of-contents__link toc-highlight"><code>gpu-copy-bw</code></a></li><li><a href="#ib-loopback" class="table-of-contents__link toc-highlight"><code>ib-loopback</code></a></li><li><a href="#nccl-bw--rccl-bw" class="table-of-contents__link toc-highlight"><code>nccl-bw</code> / <code>rccl-bw</code></a></li><li><a href="#tcp-connectivity" class="table-of-contents__link toc-highlight"><code>tcp-connectivity</code></a></li><li><a href="#gpcnet-network-test--gpcnet-network-load-test" class="table-of-contents__link toc-highlight"><code>gpcnet-network-test</code> / <code>gpcnet-network-load-test</code></a></li><li><a href="#ib-traffic" class="table-of-contents__link toc-highlight"><code>ib-traffic</code></a></li></ul></li><li><a href="#computation-communication-benchmarks" class="table-of-contents__link toc-highlight">Computation-communication Benchmarks</a><ul><li><a href="#computation-communication-overlap" class="table-of-contents__link toc-highlight"><code>computation-communication-overlap</code></a></li><li><a href="#sharding-matmul" class="table-of-contents__link toc-highlight"><code>sharding-matmul</code></a></li></ul></li><li><a href="#storage-benchmarks" class="table-of-contents__link toc-highlight">Storage Benchmarks</a><ul><li><a href="#disk-benchmark" class="table-of-contents__link toc-highlight"><code>disk-benchmark</code></a></li></ul></li><li><a href="#pytorch-model-benchmarks" class="table-of-contents__link toc-highlight">PyTorch Model Benchmarks</a><ul><li><a href="#gpt_models" class="table-of-contents__link toc-highlight"><code>gpt_models</code></a></li><li><a href="#bert_models" class="table-of-contents__link toc-highlight"><code>bert_models</code></a></li><li><a href="#lstm_models" class="table-of-contents__link toc-highlight"><code>lstm_models</code></a></li><li><a href="#cnn_models" class="table-of-contents__link toc-highlight"><code>cnn_models</code></a></li></ul></li><li><a href="#id-docker-benchmarks" class="table-of-contents__link toc-highlight">id: docker-benchmarks</a></li><li><a href="#rocm-onnxruntime-model-benchmarks" class="table-of-contents__link toc-highlight">ROCm ONNXRuntime Model Benchmarks</a><ul><li><a href="#ort-models" class="table-of-contents__link toc-highlight"><code>ort-models</code></a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/VirtualClient/docs/overview/">Overview</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/microsoft/VirtualClient/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discussion<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/VirtualClient/blog/">Blog</a></li><li class="footer__item"><a href="https://github.com/microsoft/VirtualClient" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://microsoft.com/" rel="noopener noreferrer" class="footerLogoLink_BH7S"><img src="/VirtualClient/img/microsoft-logo-white-text.svg" alt="MSFT Logo" class="themedImage_ToTc themedImage--light_HNdA footer__logo"><img src="/VirtualClient/img/microsoft-logo-white-text.svg" alt="MSFT Logo" class="themedImage_ToTc themedImage--dark_i4oU footer__logo"></a></div><div class="footer__copyright">Copyright © 2022 Microsoft.</div></div></div></footer></div>
<script src="/VirtualClient/assets/js/runtime~main.6a9313fa.js"></script>
<script src="/VirtualClient/assets/js/main.8bcee928.js"></script>
</body>
</html>